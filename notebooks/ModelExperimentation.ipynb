{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import string\n",
    "\n",
    "import mlflow\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLFLOW_TRACKING_URI: /Users/mamdouh_malaa/Developer/others/Faheem/tmp/mlflow\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = Path().resolve().parents[0]\n",
    "MODEL_REGISTRY = Path(\"/tmp/mlflow\")\n",
    "Path(MODEL_REGISTRY).mkdir(parents=True, exist_ok=True)\n",
    "MLFLOW_TRACKING_URI = str(ROOT_DIR) + str(MODEL_REGISTRY.absolute())\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "print(f\"MLFLOW_TRACKING_URI: {MLFLOW_TRACKING_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = {\n",
    "    \"test_arabic_negative_tweets\": \"../data/test_arabic_negative_tweets.tsv\",\n",
    "    \"test_arabic_positive_tweets\": \"../data/test_arabic_positive_tweets.tsv\",\n",
    "    \"train_arabic_negative_tweets\": \"../data/train_arabic_negative_tweets.tsv\",\n",
    "    \"train_arabic_positive_tweets\": \"../data/train_arabic_positive_tweets.tsv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str, label: int) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from a TSV file and changing it to a CSV file to easily read from.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the TSV file.\n",
    "        label (int): The label to assign to each tweet (0 for negative, 1 for positive).\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A Polars DataFrame containing tweets and their labels.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    with open(file_path, newline=\"\", encoding=\"utf-8\") as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            rows.append([row[1], label])\n",
    "    return pl.DataFrame(rows, schema=[\"tweet\", \"label\"])\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess a tweet by removing mentions, URLs, punctuation, emojis, diacritics,\n",
    "    and extra spaces, normalizing Arabic text, and removing repeated characters.\n",
    "\n",
    "    Args:\n",
    "        text (str): The tweet text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned and normalized tweet text.\n",
    "    \"\"\"\n",
    "    arabic_diacritics = re.compile(\n",
    "        \"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "\n",
    "                         \"\"\",\n",
    "        re.VERBOSE,\n",
    "    )\n",
    "\n",
    "    arabic_punctuations = \"\"\"`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ\"\"\"\n",
    "    english_punctuations = string.punctuation\n",
    "    punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "\n",
    "    text = re.sub(arabic_diacritics, \"\", text)\n",
    "\n",
    "    translator = str.maketrans(\"\", \"\", punctuations_list)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    text = re.sub(r\"(.)\\1+\", r\"\\1\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>tweet</th><th>label</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;كسب العيش بالحلال صعب 🌚&quot;</td><td>0</td></tr><tr><td>&quot;الحمد لله .. كانت مباراه صعبه …</td><td>1</td></tr><tr><td>&quot;احلى من اقضي اجازتي وياها 💛&quot;</td><td>1</td></tr><tr><td>&quot;انت الخير ، فوزو اليوم 😒&quot;</td><td>0</td></tr><tr><td>&quot;عشرات القتلي والجرحي في #طرابل…</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌─────────────────────────────────┬───────┐\n",
       "│ tweet                           ┆ label │\n",
       "│ ---                             ┆ ---   │\n",
       "│ str                             ┆ i64   │\n",
       "╞═════════════════════════════════╪═══════╡\n",
       "│ كسب العيش بالحلال صعب 🌚        ┆ 0     │\n",
       "│ الحمد لله .. كانت مباراه صعبه … ┆ 1     │\n",
       "│ احلى من اقضي اجازتي وياها 💛    ┆ 1     │\n",
       "│ انت الخير ، فوزو اليوم 😒       ┆ 0     │\n",
       "│ عشرات القتلي والجرحي في #طرابل… ┆ 1     │\n",
       "└─────────────────────────────────┴───────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_neg = load_data(file_paths[\"train_arabic_negative_tweets\"], 0)\n",
    "train_pos = load_data(file_paths[\"train_arabic_positive_tweets\"], 1)\n",
    "test_neg = load_data(file_paths[\"test_arabic_negative_tweets\"], 0)\n",
    "test_pos = load_data(file_paths[\"test_arabic_positive_tweets\"], 1)\n",
    "\n",
    "train_df = pl.concat([train_neg, train_pos])\n",
    "test_df = pl.concat([test_neg, test_pos])\n",
    "df = pl.concat([train_df, test_df])\n",
    "df = df.sample(fraction=1, shuffle=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(\n",
    "    [\n",
    "        pl.col(\"tweet\")\n",
    "        .map_elements(preprocess_text, return_dtype=str)\n",
    "        .alias(\"cleaned_tweet\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"cleaned_tweet\"].to_numpy()\n",
    "y = df[\"label\"].to_numpy()\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, norm=\"l2\", smooth_idf=True)\n",
    "X_tfidf = tfidf.fit_transform(vectorizer.fit_transform(X)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, y, test_size=0.2, stratify=y, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred, labels):\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.savefig(\"./dump/confusion_matrix.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7787657364204595\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.79      0.78      5656\n",
      "           1       0.79      0.77      0.78      5703\n",
      "\n",
      "    accuracy                           0.78     11359\n",
      "   macro avg       0.78      0.78      0.78     11359\n",
      "weighted avg       0.78      0.78      0.78     11359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"MultinomialNB Experiment\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"model_type\", \"MultinomialNB\")\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    report = metrics.classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    for label, metrics_dict in report.items():\n",
    "        if isinstance(metrics_dict, dict):\n",
    "            for metric_name, metric_value in metrics_dict.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "    unique_labels = list(set(y_test))\n",
    "    plot_confusion_matrix(y_test, y_pred, labels=unique_labels)\n",
    "    mlflow.log_artifact(\"./dump/confusion_matrix.png\")\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Classification Report:\\n{metrics.classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(vectorizer.transform([\"انا سعيد جدا\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(vectorizer.transform([\"الله يقلع ام الهلال خرب ام الدوري\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./models/nb_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faheem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
