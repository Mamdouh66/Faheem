{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import string\n",
    "\n",
    "import mlflow\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = Path().resolve().parents[0]\n",
    "MODEL_REGISTRY = Path(\"/tmp/mlflow\")\n",
    "Path(MODEL_REGISTRY).mkdir(parents=True, exist_ok=True)\n",
    "MLFLOW_TRACKING_URI = str(ROOT_DIR) + str(MODEL_REGISTRY.absolute())\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "print(f\"MLFLOW_TRACKING_URI: {MLFLOW_TRACKING_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = {\n",
    "    \"test_arabic_negative_tweets\": \"../data/test_arabic_negative_tweets.tsv\",\n",
    "    \"test_arabic_positive_tweets\": \"../data/test_arabic_positive_tweets.tsv\",\n",
    "    \"train_arabic_negative_tweets\": \"../data/train_arabic_negative_tweets.tsv\",\n",
    "    \"train_arabic_positive_tweets\": \"../data/train_arabic_positive_tweets.tsv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str, label: int) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from a TSV file and changing it to a CSV file to easily read from.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the TSV file.\n",
    "        label (int): The label to assign to each tweet (0 for negative, 1 for positive).\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A Polars DataFrame containing tweets and their labels.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    with open(file_path, newline=\"\", encoding=\"utf-8\") as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            rows.append([row[1], label])\n",
    "    return pl.DataFrame(rows, schema=[\"tweet\", \"label\"])\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess a tweet by removing mentions, URLs, punctuation, emojis, diacritics,\n",
    "    and extra spaces, normalizing Arabic text, and removing repeated characters.\n",
    "\n",
    "    Args:\n",
    "        text (str): The tweet text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned and normalized tweet text.\n",
    "    \"\"\"\n",
    "    arabic_diacritics = re.compile(\n",
    "        \"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "\n",
    "                         \"\"\",\n",
    "        re.VERBOSE,\n",
    "    )\n",
    "\n",
    "    arabic_punctuations = \"\"\"`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ\"\"\"\n",
    "    english_punctuations = string.punctuation\n",
    "    punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "\n",
    "    text = re.sub(arabic_diacritics, \"\", text)\n",
    "\n",
    "    translator = str.maketrans(\"\", \"\", punctuations_list)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    text = re.sub(r\"(.)\\1+\", r\"\\1\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg = load_data(file_paths[\"train_arabic_negative_tweets\"], 0)\n",
    "train_pos = load_data(file_paths[\"train_arabic_positive_tweets\"], 1)\n",
    "test_neg = load_data(file_paths[\"test_arabic_negative_tweets\"], 0)\n",
    "test_pos = load_data(file_paths[\"test_arabic_positive_tweets\"], 1)\n",
    "\n",
    "train_df = pl.concat([train_neg, train_pos])\n",
    "test_df = pl.concat([test_neg, test_pos])\n",
    "df = pl.concat([train_df, test_df])\n",
    "df = df.sample(fraction=1, shuffle=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(\n",
    "    [\n",
    "        pl.col(\"tweet\")\n",
    "        .map_elements(preprocess_text, return_dtype=str)\n",
    "        .alias(\"cleaned_tweet\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"cleaned_tweet\"].to_numpy()\n",
    "y = df[\"label\"].to_numpy()\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, norm=\"l2\", smooth_idf=True)\n",
    "X_tfidf = tfidf.fit_transform(vectorizer.fit_transform(X)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, y, test_size=0.2, stratify=y, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred, labels):\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.savefig(\"./dump/confusion_matrix.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"MultinomialNB Experiment\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"model_type\", \"MultinomialNB\")\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    report = metrics.classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    for label, metrics_dict in report.items():\n",
    "        if isinstance(metrics_dict, dict):\n",
    "            for metric_name, metric_value in metrics_dict.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "    unique_labels = list(set(y_test))\n",
    "    plot_confusion_matrix(y_test, y_pred, labels=unique_labels)\n",
    "    mlflow.log_artifact(\"./dump/confusion_matrix.png\")\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Classification Report:\\n{metrics.classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(vectorizer.transform([\"انا سعيد جدا\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(vectorizer.transform([\"الله يقلع ام الهلال خرب ام الدوري\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./models/nb_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump((vectorizer, tfidf, model), file)\n",
    "\n",
    "# with open(\"./models/vectorizer.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(vectorizer, file)\n",
    "\n",
    "# with open(\"./models/tfidf_transformer.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(tfidf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faheem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
