{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import string\n",
    "\n",
    "import mlflow\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = Path().resolve().parents[0]\n",
    "MODEL_REGISTRY = Path(\"/tmp/mlflow\")\n",
    "Path(MODEL_REGISTRY).mkdir(parents=True, exist_ok=True)\n",
    "MLFLOW_TRACKING_URI = str(ROOT_DIR) + str(MODEL_REGISTRY.absolute())\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "# print(f\"MLFLOW_TRACKING_URI: {MLFLOW_TRACKING_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = {\n",
    "    \"test_arabic_negative_tweets\": \"../data/test_arabic_negative_tweets.tsv\",\n",
    "    \"test_arabic_positive_tweets\": \"../data/test_arabic_positive_tweets.tsv\",\n",
    "    \"train_arabic_negative_tweets\": \"../data/train_arabic_negative_tweets.tsv\",\n",
    "    \"train_arabic_positive_tweets\": \"../data/train_arabic_positive_tweets.tsv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str, label: int) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from a TSV file and changing it to a CSV file to easily read from.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the TSV file.\n",
    "        label (int): The label to assign to each tweet (0 for negative, 1 for positive).\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A Polars DataFrame containing tweets and their labels.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    with open(file_path, newline=\"\", encoding=\"utf-8\") as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            rows.append([row[1], label])\n",
    "    return pl.DataFrame(rows, schema=[\"tweet\", \"label\"])\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess a tweet by removing mentions, URLs, punctuation, emojis, diacritics,\n",
    "    and extra spaces, normalizing Arabic text, and removing repeated characters.\n",
    "\n",
    "    Args:\n",
    "        text (str): The tweet text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned and normalized tweet text.\n",
    "    \"\"\"\n",
    "    arabic_diacritics = re.compile(\n",
    "        \"\"\"\n",
    "                             Ù‘    | # Tashdid\n",
    "                             Ù    | # Fatha\n",
    "                             Ù‹    | # Tanwin Fath\n",
    "                             Ù    | # Damma\n",
    "                             ÙŒ    | # Tanwin Damm\n",
    "                             Ù    | # Kasra\n",
    "                             Ù    | # Tanwin Kasr\n",
    "                             Ù’    | # Sukun\n",
    "                             Ù€     # Tatwil/Kashida\n",
    "\n",
    "                         \"\"\",\n",
    "        re.VERBOSE,\n",
    "    )\n",
    "\n",
    "    arabic_punctuations = \"\"\"`Ã·Ã—Ø›<>_()*&^%][Ù€ØŒ/:\"ØŸ.,'{}~Â¦+|!â€â€¦â€œâ€“Ù€\"\"\"\n",
    "    english_punctuations = string.punctuation\n",
    "    punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    text = re.sub(\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\", text)\n",
    "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
    "    text = re.sub(\"Ø¤\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø¦\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
    "    text = re.sub(\"Ú¯\", \"Ùƒ\", text)\n",
    "\n",
    "    text = re.sub(arabic_diacritics, \"\", text)\n",
    "\n",
    "    translator = str.maketrans(\"\", \"\", punctuations_list)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    text = re.sub(r\"(.)\\1+\", r\"\\1\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>tweet</th><th>label</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;ÙƒÙ„ Ø§Ù„ÙŠ ØªÙ…Ù†ÙŠÙ†Ù‡ ÙŠØ§ ÙÙ„Ø§Ù†Ù‡ Ø§Ù† ÙÙŠ Ø´â€¦</td><td>1</td></tr><tr><td>&quot;Ø±Ø­Ù…Ø© Ø§Ù„Ù„Ù‡ Ø§Ù„ÙˆØ§Ø³Ø¹Ø© Ø¹Ù„ÙŠÙ‡ Ø±Ø¨ÙŠ ÙŠØ¬Ø¹â€¦</td><td>1</td></tr><tr><td>&quot;ÙÙŠÙ‡Ø§ ÙƒØ±Øª Ø£ØµÙØ± Ø«Ø§Ù†ÙŠ .. Ø¨Ø¹Ø¯ Ø§ØµØ§Ø¨â€¦</td><td>1</td></tr><tr><td>&quot;ØªÙ„Ø§ØªÙŠÙ† Ø³Ù†Ø© Ø¨ØªØ±Ù‚Øµ .. Ø§Ù„Ù„ÙŠÙ„Ø© Ø±Ù‚Øµâ€¦</td><td>0</td></tr><tr><td>&quot;ØªÙ…ÙˆØªØª ğŸ˜­ğŸ’”ğŸ’” Ø§Ù„Ù„ÙŠ Ø­Ø§Ø¨Ù‡ ØªØ³ÙˆÙŠ Ø§Ø¶Ø§Ø¡Ø©â€¦</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ tweet                           â”† label â”‚\n",
       "â”‚ ---                             â”† ---   â”‚\n",
       "â”‚ str                             â”† i64   â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
       "â”‚ ÙƒÙ„ Ø§Ù„ÙŠ ØªÙ…Ù†ÙŠÙ†Ù‡ ÙŠØ§ ÙÙ„Ø§Ù†Ù‡ Ø§Ù† ÙÙŠ Ø´â€¦ â”† 1     â”‚\n",
       "â”‚ Ø±Ø­Ù…Ø© Ø§Ù„Ù„Ù‡ Ø§Ù„ÙˆØ§Ø³Ø¹Ø© Ø¹Ù„ÙŠÙ‡ Ø±Ø¨ÙŠ ÙŠØ¬Ø¹â€¦ â”† 1     â”‚\n",
       "â”‚ ÙÙŠÙ‡Ø§ ÙƒØ±Øª Ø£ØµÙØ± Ø«Ø§Ù†ÙŠ .. Ø¨Ø¹Ø¯ Ø§ØµØ§Ø¨â€¦ â”† 1     â”‚\n",
       "â”‚ ØªÙ„Ø§ØªÙŠÙ† Ø³Ù†Ø© Ø¨ØªØ±Ù‚Øµ .. Ø§Ù„Ù„ÙŠÙ„Ø© Ø±Ù‚Øµâ€¦ â”† 0     â”‚\n",
       "â”‚ ØªÙ…ÙˆØªØª ğŸ˜­ğŸ’”ğŸ’” Ø§Ù„Ù„ÙŠ Ø­Ø§Ø¨Ù‡ ØªØ³ÙˆÙŠ     â”† 1     â”‚\n",
       "â”‚ Ø§Ø¶Ø§Ø¡Ø©â€¦                          â”†       â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_neg = load_data(file_paths[\"train_arabic_negative_tweets\"], 0)\n",
    "train_pos = load_data(file_paths[\"train_arabic_positive_tweets\"], 1)\n",
    "test_neg = load_data(file_paths[\"test_arabic_negative_tweets\"], 0)\n",
    "test_pos = load_data(file_paths[\"test_arabic_positive_tweets\"], 1)\n",
    "\n",
    "train_df = pl.concat([train_neg, train_pos])\n",
    "test_df = pl.concat([test_neg, test_pos])\n",
    "df = pl.concat([train_df, test_df])\n",
    "df = df.sample(fraction=1, shuffle=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(\n",
    "    [\n",
    "        pl.col(\"tweet\")\n",
    "        .map_elements(preprocess_text, return_dtype=str)\n",
    "        .alias(\"cleaned_tweet\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>label</th></tr><tr><td>u32</td></tr></thead><tbody><tr><td>28513</td></tr><tr><td>28282</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2,)\n",
       "Series: 'label' [u32]\n",
       "[\n",
       "\t28513\n",
       "\t28282\n",
       "]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].unique_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"cleaned_tweet\"].to_numpy()\n",
    "y = df[\"label\"].to_numpy()\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, norm=\"l2\", smooth_idf=True)\n",
    "X_tfidf = tfidf.fit_transform(vectorizer.fit_transform(X)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, y, test_size=0.2, stratify=y, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred, labels):\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.savefig(\"./dump/confusion_matrix.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7823752090853068\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.80      0.79      5656\n",
      "           1       0.80      0.76      0.78      5703\n",
      "\n",
      "    accuracy                           0.78     11359\n",
      "   macro avg       0.78      0.78      0.78     11359\n",
      "weighted avg       0.78      0.78      0.78     11359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"MultinomialNB Experiment\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"model_type\", \"MultinomialNB\")\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    report = metrics.classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    for label, metrics_dict in report.items():\n",
    "        if isinstance(metrics_dict, dict):\n",
    "            for metric_name, metric_value in metrics_dict.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "    unique_labels = list(set(y_test))\n",
    "    plot_confusion_matrix(y_test, y_pred, labels=unique_labels)\n",
    "    mlflow.log_artifact(\"./dump/confusion_matrix.png\")\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Classification Report:\\n{metrics.classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proper_format(text: str):\n",
    "    text = preprocess_text(text)\n",
    "    vectorized_text = vectorizer.transform([text])\n",
    "    transformed_text = tfidf.transform(vectorized_text)\n",
    "    return transformed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(get_proper_format(\"ÙˆÙ„Ø§Ù† Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø§Ù„ ÙŠØµØ¹Ø¨ Ø´Ø±Ø­Ù‡ Ø§Ø«Ø±Øª ØµÙ…ØªØ§ ÙˆØ§Ù„Ø³ÙƒÙˆØª Ù…Ø±ÙŠØ±\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(get_proper_format(\"Ø§Ù„Ù„Ù‡ ÙŠÙ‚Ù„Ø¹ Ø§Ù… Ø§Ù„Ù‡Ù„Ø§Ù„ Ø®Ø±Ø¨ Ø§Ù… Ø§Ù„Ø¯ÙˆØ±ÙŠ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.predict(get_proper_format(\"Ø§Ù†Ø³Øª Ø¨ÙˆØ­Ø¯ØªÙŠ ÙˆÙ„Ø²Ù…Øª Ø¨ÙŠØªÙŠ ÙØ·Ø§Ø¨ Ø§Ù„Ø§Ù†Ø³ Ù„ÙŠ ÙˆØµÙØ§ Ø§Ù„Ø³Ø±ÙˆØ±\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./models/nb_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump((vectorizer, tfidf, model), file)\n",
    "\n",
    "# with open(\"./models/vectorizer.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(vectorizer, file)\n",
    "\n",
    "# with open(\"./models/tfidf_transformer.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(tfidf, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faheem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
